{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_num = 0\n",
    "url = \"http://bbs.tianya.cn/list.jsp?item=free&nextid=%d&order=8&k=PX\" % page_num\n",
    "content = urllib2.urlopen(url).read() #获取网页的html文本\n",
    "soup = BeautifulSoup(content, \"lxml\") \n",
    "articles = soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr class=\"bg\">\n",
      "<td class=\"td-title \">\n",
      "<span class=\"face\" title=\"\">\n",
      "</span>\n",
      "<a href=\"/post-free-2849477-1.shtml\" target=\"_blank\">\r\n",
      "\t\t\t\t\t\t\t【民间语文第161期】宁波px启示:船进港湾人应上岸<span class=\"art-ico art-ico-3\" title=\"内有0张图片\"></span>\n",
      "</a>\n",
      "</td>\n",
      "<td><a class=\"author\" href=\"http://www.tianya.cn/50499450\" target=\"_blank\">贾也</a></td>\n",
      "<td>194704</td>\n",
      "<td>2703</td>\n",
      "<td title=\"2012-10-29 07:59\">10-29 07:59</td>\n",
      "</tr>\n"
     ]
    }
   ],
   "source": [
    "print articles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td=articles[1].find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【民间语文第161期】宁波px启示:船进港湾人应上岸\n"
     ]
    }
   ],
   "source": [
    "print td[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/post-free-2849477-1.shtml\n"
     ]
    }
   ],
   "source": [
    "print td[0].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td><a class=\"author\" href=\"http://www.tianya.cn/50499450\" target=\"_blank\">贾也</a></td>\n"
     ]
    }
   ],
   "source": [
    "print td[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in articles[1:]:\n",
    "    td = i.find_all('td')\n",
    "    title = td[0].text.strip()\n",
    "    title_url = td[0].a['href']\n",
    "    author = td[1].text\n",
    "    author_url = td[1].a['href']\n",
    "    views = td[2].text\n",
    "    replies = td[3].text\n",
    "    date = td[4]['title']\n",
    "    record = title + '\\t' + title_url+ '\\t' + author + '\\t'+ author_url + '\\t' + views+ '\\t'  + replies+ '\\t'+ date\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【民间语文第161期】宁波px启示:船进港湾人应上岸\t/post-free-2849477-1.shtml\t贾也\thttp://www.tianya.cn/50499450\t194704\t2703\t2012-10-29 07:59\n"
     ]
    }
   ],
   "source": [
    "print records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawler(page_num, file_name):\n",
    "    try:\n",
    "        # open the browser\n",
    "        url = \"http://bbs.tianya.cn/list.jsp?item=free&nextid=%d&order=8&k=PX\" % page_num\n",
    "        content = urllib2.urlopen(url).read() #获取网页的html文本\n",
    "        soup = BeautifulSoup(content, \"lxml\") \n",
    "        articles = soup.find_all('tr')\n",
    "        # write down info\n",
    "        for i in articles[1:]:\n",
    "            td = i.find_all('td')\n",
    "            title = td[0].text.strip()\n",
    "            title_url = td[0].a['href']\n",
    "            author = td[1].text\n",
    "            author_url = td[1].a['href']\n",
    "            views = td[2].text\n",
    "            replies = td[3].text\n",
    "            date = td[4]['title']\n",
    "            record = title + '\\t' + title_url+ '\\t' + author + '\\t'+ \\\n",
    "                        author_url + '\\t' + views+ '\\t'  + replies+ '\\t'+ date\n",
    "            with open(file_name,'a') as p: # '''Note'''：Ａppend mode, run only once!\n",
    "                        p.write(record.encode('utf-8')+\"\\n\") ##!!encode here to utf-8 to avoid encoding\n",
    "\n",
    "    except Exception, e:\n",
    "        print e\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for page_num in range(10):\n",
    "    print (page_num)\n",
    "    crawler(page_num,'/Users/Oran/Desktop/github/1.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【民间语文第161期】宁波px启示:船进港湾人应上岸</td>\n",
       "      <td>/post-free-2849477-1.shtml</td>\n",
       "      <td>贾也</td>\n",
       "      <td>http://www.tianya.cn/50499450</td>\n",
       "      <td>194704</td>\n",
       "      <td>2703</td>\n",
       "      <td>2012-10-29 07:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>宁波镇海PX项目引发群体上访 当地政府发布说明(转载)</td>\n",
       "      <td>/post-free-2839539-1.shtml</td>\n",
       "      <td>无上卫士ABC</td>\n",
       "      <td>http://www.tianya.cn/74341835</td>\n",
       "      <td>88258</td>\n",
       "      <td>1041</td>\n",
       "      <td>2012-10-24 12:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0                           1        2  \\\n",
       "0   【民间语文第161期】宁波px启示:船进港湾人应上岸  /post-free-2849477-1.shtml       贾也   \n",
       "1  宁波镇海PX项目引发群体上访 当地政府发布说明(转载)  /post-free-2839539-1.shtml  无上卫士ABC   \n",
       "\n",
       "                               3       4     5                 6  \n",
       "0  http://www.tianya.cn/50499450  194704  2703  2012-10-29 07:59  \n",
       "1  http://www.tianya.cn/74341835   88258  1041  2012-10-24 12:41  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/Oran/Desktop/github/1.txt', sep = \"\\t\", header=None)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>author_page</th>\n",
       "      <th>click</th>\n",
       "      <th>reply</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【民间语文第161期】宁波px启示:船进港湾人应上岸</td>\n",
       "      <td>/post-free-2849477-1.shtml</td>\n",
       "      <td>贾也</td>\n",
       "      <td>http://www.tianya.cn/50499450</td>\n",
       "      <td>194704</td>\n",
       "      <td>2703</td>\n",
       "      <td>2012-10-29 07:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>宁波镇海PX项目引发群体上访 当地政府发布说明(转载)</td>\n",
       "      <td>/post-free-2839539-1.shtml</td>\n",
       "      <td>无上卫士ABC</td>\n",
       "      <td>http://www.tianya.cn/74341835</td>\n",
       "      <td>88258</td>\n",
       "      <td>1041</td>\n",
       "      <td>2012-10-24 12:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>宁波准备停止PX项目了,元芳,你怎么看?</td>\n",
       "      <td>/post-free-2848797-1.shtml</td>\n",
       "      <td>牧阳光</td>\n",
       "      <td>http://www.tianya.cn/36535656</td>\n",
       "      <td>83032</td>\n",
       "      <td>625</td>\n",
       "      <td>2012-10-28 19:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title                        link   author  \\\n",
       "0   【民间语文第161期】宁波px启示:船进港湾人应上岸  /post-free-2849477-1.shtml       贾也   \n",
       "1  宁波镇海PX项目引发群体上访 当地政府发布说明(转载)  /post-free-2839539-1.shtml  无上卫士ABC   \n",
       "2         宁波准备停止PX项目了,元芳,你怎么看?  /post-free-2848797-1.shtml      牧阳光   \n",
       "\n",
       "                     author_page   click  reply              time  \n",
       "0  http://www.tianya.cn/50499450  194704   2703  2012-10-29 07:59  \n",
       "1  http://www.tianya.cn/74341835   88258   1041  2012-10-24 12:41  \n",
       "2  http://www.tianya.cn/36535656   83032    625  2012-10-28 19:11  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.rename(columns = {0:'title', 1:'link', 2:'author',3:'author_page', 4:'click', 5:'reply', 6:'time'})\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    http://www.tianya.cn/50499450\n",
       "1    http://www.tianya.cn/74341835\n",
       "2    http://www.tianya.cn/36535656\n",
       "3    http://www.tianya.cn/36959960\n",
       "4    http://www.tianya.cn/53134970\n",
       "Name: author_page, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author_page[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = urllib2.urlopen('http://www.tianya.cn/62237033').read() \n",
    "soup = BeautifulSoup(content, \"lxml\")\n",
    "link_info = soup.find_all('div', {'class', 'link-box'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"link-box\"><p><a href=\"/62237033/follow\">0</a></p><p>关注</p></div>\n",
      "<div class=\"link-box\"><p><a href=\"/62237033/fans\">0</a></p><p>粉丝</p></div>\n"
     ]
    }
   ],
   "source": [
    "for t in link_info:\n",
    "    print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"subtitle\"><a href=\"http://www.tianya.cn/112521680/bbs?t=post\">\\u4e3b\\u5e161</a>\\u3000<a href=\"http://www.tianya.cn/112521680/bbs?t=reply\">\\u56de\\u5e160</a></span>]\n"
     ]
    }
   ],
   "source": [
    "content = urllib2.urlopen('http://www.tianya.cn/112521680').read() \n",
    "soup = BeautifulSoup(content, \"lxml\")\n",
    "activity = soup.find_all('span', {'class', 'subtitle'})\n",
    "print activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def author_crawler(url, file_name):\n",
    "    try:\n",
    "        content = urllib2.urlopen(url).read() #获取网页的html文本\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        link_info = soup.find_all('div', {'class', 'link-box'})\n",
    "        followed_num, fans_num = [i.a.text for i in link_info]\n",
    "        try:\n",
    "            activity = soup.find_all('span', {'class', 'subtitle'})\n",
    "            post_num, reply_num = [j.text[2:] for i in activity[:1] for j in i('a')]\n",
    "        except:\n",
    "            post_num, reply_num = 1, 0\n",
    "        record =  '\\t'.join([url, followed_num, fans_num, post_num, reply_num])\n",
    "        with open(file_name,'a') as p: # '''Note'''：Ａppend mode, run only once!\n",
    "                    p.write(record.encode('utf-8')+\"\\n\") ##!!encode here to utf-8 to avoid encoding\n",
    "\n",
    "    except Exception, e:\n",
    "        print e, url\n",
    "        record =  '\\t'.join([url, 'na', 'na', 'na', 'na'])\n",
    "        with open(file_name,'a') as p: # '''Note'''：Ａppend mode, run only once!\n",
    "                    p.write(record.encode('utf-8')+\"\\n\") ##!!encode here to utf-8 to avoid encoding\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n",
      "need more than 0 values to unpack http://www.tianya.cn/42330613\n",
      "sequence item 3: expected string or Unicode, int found http://www.tianya.cn/26517664\n",
      "need more than 0 values to unpack http://www.tianya.cn/75591747\n",
      "need more than 0 values to unpack http://www.tianya.cn/24068399\n",
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n",
      "sequence item 3: expected string or Unicode, int found http://www.tianya.cn/62237033\n",
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n",
      "need more than 0 values to unpack http://www.tianya.cn/85353911\n",
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n",
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n",
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n",
      "need more than 0 values to unpack http://www.tianya.cn/67896263\n"
     ]
    }
   ],
   "source": [
    "for url in (df.author_page):\n",
    "    author_crawler(url, '/Users/Oran/Desktop/github/2.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/post-free-2848797-1.shtml'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.link[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://bbs.tianya.cn/post-free-2848797-1.shtml'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://bbs.tianya.cn' + df.link[2]\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = urllib2.urlopen(url).read() \n",
    "post_soup = BeautifulSoup(post, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa = post_soup.find_all('div', {'class', 'atl-item'})\n",
    "len(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div _host=\"%E7%89%A7%E9%98%B3%E5%85%89\" class=\"atl-item host-item\">\n",
      "<div class=\"atl-content\">\n",
      "<div class=\"atl-con-hd clearfix\">\n",
      "<div class=\"atl-con-hd-l\"></div>\n",
      "<div class=\"atl-con-hd-r\"></div>\n",
      "</div>\n",
      "<div class=\"atl-con-bd clearfix\">\n",
      "<div class=\"bbs-content clearfix\">\n",
      "<br/>\n",
      "　　从宁波市政府新闻发言人处获悉，宁波市经与项目投资方研究决定：（1）坚决不上PX项目；（2）炼化一体化项目前期工作停止推进，再作科学论证。<br/>\n",
      "<br/>\n",
      "</div>\n",
      "<div class=\"clearfix\" id=\"alt_action\"></div>\n",
      "<div class=\"clearfix\">\n",
      "<div class=\"host-data\">\n",
      "<span>楼主发言：11次</span> <span>发图：0张</span>\n",
      "</div>\n",
      "<div class=\"atl-reply\" id=\"alt_reply\">\n",
      "<a author=\"牧阳光\" authorid=\"36535656\" class=\"reportme a-link\" href=\"javascript:void(0);\" replyid=\"0\" replytime=\"2012-10-28 19:11:00\"> 举报</a> | \r\n",
      "\t\t\t\t\t\t\t<a class=\"a-link acl-share\" href=\"javascript:void(0);\">分享</a> |  \r\n",
      "\t\t\t               \t<a class=\"a-link acl-more\" href=\"javascript:void(0);\">更多</a> |\r\n",
      "\t\t\t\t\t\t\t<span><a class=\"a-link\" name=\"0\">楼主</a></span>\n",
      "<a _name=\"牧阳光\" _time=\"2012-10-28 19:11:00\" class=\"a-link2 replytop\" href=\"#fabu_anchor\">回复</a>\n",
      "</div>\n",
      "</div>\n",
      "<div id=\"ds-quick-box\" style=\"display:none;\"></div>\n",
      "</div>\n",
      "<div class=\"atl-con-ft clearfix\">\n",
      "<div class=\"atl-con-ft-l\"></div>\n",
      "<div class=\"atl-con-ft-r\"></div>\n",
      "<div id=\"niuren_ifm\"></div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print pa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"atl-pages\"><form action=\"\" method=\"get\" onsubmit=\"return goPage(this,'free',2848797,7);\">\\n<span>\\u4e0a\\u9875</span>\\n<strong>1</strong>\\n<a href=\"/post-free-2848797-2.shtml\">2</a>\\n<a href=\"/post-free-2848797-3.shtml\">3</a>\\n<a href=\"/post-free-2848797-4.shtml\">4</a>\\n\\u2026\\n<a href=\"/post-free-2848797-7.shtml\">7</a>\\n<a class=\"js-keyboard-next\" href=\"/post-free-2848797-2.shtml\">\\u4e0b\\u9875</a>\\n\\xa0\\u5230<input class=\"pagetxt\" name=\"page\" type=\"text\"/>\\u9875\\xa0<input class=\"pagebtn\" maxlength=\"6\" name=\"gopage\" type=\"submit\" value=\"\\u786e\\u5b9a\"/></form></div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_soup.find('div', {'class', 'atl-pages'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_pages = post_soup.find('div', {'class', 'atl-pages'})\n",
    "post_pages = post_pages.form['onsubmit'].split(',')[-1].split(')')[0]\n",
    "post_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://bbs.tianya.cn/postfree2848797-%d.shtml'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url = 'http://bbs.tianya.cn' + df.link[2]\n",
    "url_base = ''.join(url.split('-')[:-1]) + '-%d.shtml'\n",
    "url_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePage(pa):\n",
    "    records = []\n",
    "    for i in pa:\n",
    "        p_info = i.find('a', class_ = 'reportme a-link')\n",
    "        p_time = p_info['replytime']\n",
    "        p_author_id = p_info['authorid']\n",
    "        p_author_name = p_info['author']\n",
    "        p_content = i.find('div', {'class', 'bbs-content'}).text.strip()\n",
    "        p_content = p_content.replace('\\t', '').replace('\\n', '')#.replace(' ', '')\n",
    "        record = p_time + '\\t' + p_author_id+ '\\t' + p_author_name + '\\t'+ p_content\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "import sys\n",
    "def flushPrint(s):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('%s' % s)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def crawler(url, file_name):\n",
    "    try:\n",
    "        # open the browser\n",
    "        url_1 = 'http://bbs.tianya.cn' + url\n",
    "        content = urllib2.urlopen(url_1).read() \n",
    "        post_soup = BeautifulSoup(content, \"lxml\") \n",
    "        # how many pages in a post\n",
    "        post_form = post_soup.find('div', {'class', 'atl-pages'})\n",
    "        if post_form.form:\n",
    "            post_pages = post_form.form['onsubmit'].split(',')[-1].split(')')[0]\n",
    "            post_pages = int(post_pages)\n",
    "            url_base = '-'.join(url_1.split('-')[:-1]) + '-%d.shtml'\n",
    "        else:\n",
    "            post_pages = 1\n",
    "        # for the first page\n",
    "        pa = post_soup.find_all('div', {'class', 'atl-item'})\n",
    "        records = parsePage(pa)\n",
    "        with open(file_name,'a') as p: # '''Note'''：Ａppend mode, run only once!\n",
    "            for record in records:    \n",
    "                p.write('1'+ '\\t' + url + '\\t' + record.encode('utf-8')+\"\\n\") \n",
    "        # for the 2nd+ pages\n",
    "        if post_pages > 1:\n",
    "            for page_num in range(2, post_pages+1):\n",
    "                time.sleep(random.random())\n",
    "                flushPrint(page_num)\n",
    "                url2 =url_base  % page_num\n",
    "                content = urllib2.urlopen(url2).read()\n",
    "                post_soup = BeautifulSoup(content, \"lxml\") \n",
    "                pa = post_soup.find_all('div', {'class', 'atl-item'})\n",
    "                records = parsePage(pa)\n",
    "                with open(file_name,'a') as p: \n",
    "                    for record in records:    \n",
    "                        p.write(str(page_num) + '\\t' +url + '\\t' + record.encode('utf-8')+\"\\n\") \n",
    "        else:\n",
    "            pass\n",
    "    except Exception, e:\n",
    "        print e\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7"
     ]
    }
   ],
   "source": [
    "url = df.link[2]\n",
    "file_name = '/Users/Oran/Desktop/github/3.txt'\n",
    "crawler(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/post-free-2849477-1.shtmlThis it the post of : 0\n",
      "/post-free-2842180-1.shtmlThis it the post of : 10\n",
      "/post-free-3316698-1.shtmlThis it the post of : 20\n",
      "/post-free-923387-1.shtmlThis it the post of : 30\n",
      "/post-free-4236026-1.shtmlThis it the post of : 40\n",
      "/post-free-2850721-1.shtmlThis it the post of : 50\n",
      "/post-free-5054821-1.shtmlThis it the post of : 60\n",
      "/post-free-3326274-1.shtmlThis it the post of : 70\n",
      "/post-free-4236793-1.shtmlThis it the post of : 80\n",
      "/post-free-919901-1.shtmlThis it the post of : 90\n",
      "/post-free-5042110-1.shtmlThis it the post of : 100\n",
      "/post-free-4241153-1.shtmlThis it the post of : 110\n",
      "/post-free-3324561-1.shtmlThis it the post of : 120\n",
      "/post-free-921701-1.shtmlThis it the post of : 130\n",
      "/post-free-5045950-1.shtmlThis it the post of : 140\n",
      "/post-free-2848818-1.shtmlThis it the post of : 150\n",
      "/post-free-3352554-1.shtmlThis it the post of : 160\n",
      "/post-free-949151-1.shtmlThis it the post of : 170\n",
      "/post-free-3833300-1.shtmlThis it the post of : 180\n",
      "/post-free-3228423-1.shtmlThis it the post of : 190\n",
      "/post-free-2852970-1.shtmlThis it the post of : 200\n",
      "/post-free-3325388-1.shtmlThis it the post of : 210\n",
      "/post-free-3835748-1.shtmlThis it the post of : 220\n",
      "/post-free-3833431-1.shtmlThis it the post of : 230\n",
      "/post-free-3378998-1.shtmlThis it the post of : 240\n",
      "/post-free-3359022-1.shtmlThis it the post of : 250\n",
      "/post-free-3291344-1.shtmlThis it the post of : 260\n",
      "/post-free-3302401-1.shtmlThis it the post of : 270\n",
      "/post-free-3371518-1.shtmlThis it the post of : 280\n",
      "/post-free-3838312-1.shtmlThis it the post of : 290\n",
      "/post-free-3835060-1.shtmlThis it the post of : 300\n",
      "/post-free-4247579-1.shtmlThis it the post of : 310\n",
      "/post-free-3834191-1.shtmlThis it the post of : 320\n",
      "/post-free-3839013-1.shtmlThis it the post of : 330\n",
      "/post-free-3833924-1.shtmlThis it the post of : 340\n",
      "/post-free-3834743-1.shtmlThis it the post of : 350\n",
      "/post-free-3834873-1.shtmlThis it the post of : 360\n",
      "/post-free-3834849-1.shtmlThis it the post of : 370\n",
      "/post-free-3838168-1.shtmlThis it the post of : 380\n",
      "/post-free-3834838-1.shtmlThis it the post of : 390\n",
      "/post-free-3295520-1.shtmlThis it the post of : 400\n",
      "/post-free-3838234-1.shtmlThis it the post of : 410\n",
      "/post-free-3834916-1.shtmlThis it the post of : 420\n",
      "/post-free-3834616-1.shtmlThis it the post of : 430\n",
      "/post-free-3844596-1.shtmlThis it the post of : 440\n",
      "/post-free-4076096-1.shtmlThis it the post of : 450\n",
      "/post-free-3837988-1.shtmlThis it the post of : 460\n",
      "/post-free-3835031-1.shtml"
     ]
    }
   ],
   "source": [
    "for k, link in enumerate(df.link):\n",
    "    flushPrint(link)\n",
    "    if k % 10== 0:\n",
    "        print 'This it the post of : ' + str(k)\n",
    "    file_name = '/Users/Oran/Desktop/github/4.txt'\n",
    "    crawler(link, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8081"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtt = []\n",
    "with open('/Users/Oran/Desktop/github/4.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        pnum, link, time, author_id, author, content = line.replace('\\n', '').split('\\t')\n",
    "        dtt.append([pnum, link, time, author_id, author, content])\n",
    "len(dtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
