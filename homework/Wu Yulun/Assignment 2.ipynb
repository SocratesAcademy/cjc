{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.hprc.org.cn/wxzl/wxysl/lczf/ width=1000 height=500></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "HTML('<iframe src=http://www.hprc.org.cn/wxzl/wxysl/lczf/ width=1000 height=500></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawler(url_i):\n",
    "    content = urllib2.urlopen(url_i).read().decode('gb18030')  \n",
    "    soup = BeautifulSoup(content, 'html.parser') \n",
    "    year = soup.find('span', {'class', 'huang16c'}).text[:4]\n",
    "    year = int(year)\n",
    "    report = ''.join(s.text for s in soup('p'))\n",
    "    scripts = soup.find_all('script')\n",
    "    countPage = int(''.join(scripts[1]).split('countPage = ')[1].split('//')[0])\n",
    "    if countPage == 1:\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(1, countPage):\n",
    "            url_child = url_i.split('.html')[0] +'_'+str(i)+'.html'\n",
    "            content = urllib2.urlopen(url_child).read().decode('gb18030') \n",
    "            soup = BeautifulSoup(content) \n",
    "            report_child = ''.join(s.text for s in soup('p'))\n",
    "            report = report + report_child\n",
    "    return year, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'./d12qgrdzfbg/201603/t20160318_369509.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.hprc.org.cn/wxzl/wxysl/lczf/\" \n",
    "content = urllib2.urlopen(url).read().decode('gb18030') \n",
    "soup = BeautifulSoup(content, 'html.parser') \n",
    "links = soup.select('.bl a')\n",
    "links[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://www.hprc.org.cn/wxzl/wxysl/lczf/d12qgrdzfbg/201603/t20160318_369509.html',\n",
       " u'http://www.hprc.org.cn/wxzl/wxysl/lczf/d12qgrdzfbg/201503/t20150318_319434.html',\n",
       " u'http://www.hprc.org.cn/wxzl/wxysl/lczf/d12qgrdzfbg/201403/t20140315_270863.html',\n",
       " u'http://www.hprc.org.cn/wxzl/wxysl/lczf/d12qgrdzfbg/201402/t20140214_266528.html',\n",
       " u'http://www.hprc.org.cn/wxzl/wxysl/lczf/dishiyijie/201402/t20140214_266527.html']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperlinks = [url + i['href'].split('./')[1] for i in links]\n",
    "hyperlinks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.hprc.org.cn/wxzl/wxysl/lczf/dishiyijie_1/200908/t20090818_27775.html width=1000 height=500></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "HTML('<iframe src=http://www.hprc.org.cn/wxzl/wxysl/lczf/dishiyijie_1/200908/t20090818_27775.html \\\n",
    "width=1000 height=500></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_i = 'http://www.hprc.org.cn/wxzl/wxysl/lczf/dishiyijie_1/200908/t20090818_27775.html'\n",
    "content = urllib2.urlopen(url_i).read().decode('gb18030')  \n",
    "soup = BeautifulSoup(content, 'html.parser') \n",
    "\n",
    "\n",
    "scripts = soup.select('td script')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<script>\\n\\tvar currentPage = 0;//\\u6240\\u5728\\u9875\\u4ece0\\u5f00\\u59cb\\n\\tvar prevPage = currentPage-1//\\u4e0a\\u4e00\\u9875\\n\\tvar \\u4e0b\\u4e00\\u9875Page = currentPage+1//\\u4e0b\\u4e00\\u9875\\n\\tvar countPage = 4//\\u5171\\u591a\\u5c11\\u9875\\n\\t//document.write(\"\\u5171\"+countPage+\"\\u9875&nbsp;&nbsp;\");\\n\\t\\n\\t//\\u5faa\\u73af\\n\\tvar num = 17;\\n\\tfor(var i=0+(currentPage-1-(currentPage-1)%num) ; i<=(num+(currentPage-1-(currentPage-1)%num))&&(i<countPage) ; i++){\\n\\t\\tif(countPage >1){\\n\\t\\t\\tif(currentPage==i)\\n\\t\\t\\t\\tdocument.write(\"\\u3010<span style=\\\\\"color:#FF0000;\\\\\" class=\\\\\"hui14_30_h\\\\\">\"+(i+1)+\"</span>\\u3011&nbsp;\");\\n\\t\\t\\telse if(i==0)\\n\\t\\t\\t\\tdocument.write(\"<a href=\\\\\"t20090818_27775.html\\\\\" class=\\\\\"hui14_30_h\\\\\">\\u3010\"+(i+1)+\"\\u3011</a>&nbsp;\");\\n\\t\\t\\telse\\n\\t\\t\\t\\tdocument.write(\"<a href=\\\\\"t20090818_27775\"+\"_\" + i + \".\"+\"html\\\\\" class=\\\\\"hui14_30_h\\\\\">\\u3010\"+(i+1)+\"\\u3011</a>&nbsp;\");\\n\\t\\t}\\t\\n\\t}\\n\\t\\n\\tdocument.write(\"<br><br>\");\\n\\t//\\u8bbe\\u7f6e\\u4e0a\\u4e00\\u9875\\u4ee3\\u7801\\n\\tif(countPage>1&&currentPage!=0&&currentPage!=1)\\n\\t\\tdocument.write(\"<a href=\\\\\"t20090818_27775\"+\"_\" + prevPage + \".\"+\"html\\\\\"><span style=\\\\\"color:#0033FF;font-weight:bold\\\\\">\\u4e0a\\u4e00\\u9875</span></a>&nbsp;\");\\n\\telse if(countPage>1&&currentPage!=0&&currentPage==1)\\n\\t\\tdocument.write(\"<a href=\\\\\"t20090818_27775.html\\\\\"><span style=\\\\\"color:#0033FF;font-weight:bold\\\\\">\\u4e0a\\u4e00\\u9875</span></a>&nbsp;\");\\n\\t//else\\n\\t//\\tdocument.write(\"\\u4e0a\\u4e00\\u9875 &nbsp;\");\\n\\t\\n\\t\\n\\t//\\u8bbe\\u7f6e\\u4e0b\\u4e00\\u9875\\u4ee3\\u7801 \\n\\tif(countPage>1&&currentPage!=(countPage-1))\\n\\t\\tdocument.write(\"<a href=\\\\\"t20090818_27775\"+\"_\" + \\u4e0b\\u4e00\\u9875Page + \".\"+\"html\\\\\" ><span style=\\\\\"color:#0033FF;font-weight:bold\\\\\">\\u4e0b\\u4e00\\u9875</span></a> &nbsp;\");\\n\\t//else\\n\\t//\\tdocument.write(\"\\u4e0b\\u4e00\\u9875 &nbsp;\");\\n\\t\\t\\t\\t\\t \\n\\t</script>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tvar currentPage = 0;//所在页从0开始\n",
      "\tvar prevPage = currentPage-1//上一页\n",
      "\tvar 下一页Page = currentPage+1//下一页\n",
      "\tvar countPage = 4//共多少页\n",
      "\t//document.write(\"共\"+countPage+\"页&nbsp;&nbsp;\");\n",
      "\t\n",
      "\t//循环\n",
      "\tvar num = 17;\n",
      "\tfor(var i=0+(currentPage-1-(currentPage-1)%num) ; i<=(num+(currentPage-1-(currentPage-1)%num))&&(i<countPage) ; i++){\n",
      "\t\tif(countPage >1){\n",
      "\t\t\tif(currentPage==i)\n",
      "\t\t\t\tdocument.write(\"【<span style=\\\"color:#FF0000;\\\" class=\\\"hui14_30_h\\\">\"+(i+1)+\"</span>】&nbsp;\");\n",
      "\t\t\telse if(i==0)\n",
      "\t\t\t\tdocument.write(\"<a href=\\\"t20090818_27775.html\\\" class=\\\"hui14_30_h\\\">【\"+(i+1)+\"】</a>&nbsp;\");\n",
      "\t\t\telse\n",
      "\t\t\t\tdocument.write(\"<a href=\\\"t20090818_27775\"+\"_\" + i + \".\"+\"html\\\" class=\\\"hui14_30_h\\\">【\"+(i+1)+\"】</a>&nbsp;\");\n",
      "\t\t}\t\n",
      "\t}\n",
      "\t\n",
      "\tdocument.write(\"<br><br>\");\n",
      "\t//设置上一页代码\n",
      "\tif(countPage>1&&currentPage!=0&&currentPage!=1)\n",
      "\t\tdocument.write(\"<a href=\\\"t20090818_27775\"+\"_\" + prevPage + \".\"+\"html\\\"><span style=\\\"color:#0033FF;font-weight:bold\\\">上一页</span></a>&nbsp;\");\n",
      "\telse if(countPage>1&&currentPage!=0&&currentPage==1)\n",
      "\t\tdocument.write(\"<a href=\\\"t20090818_27775.html\\\"><span style=\\\"color:#0033FF;font-weight:bold\\\">上一页</span></a>&nbsp;\");\n",
      "\t//else\n",
      "\t//\tdocument.write(\"上一页 &nbsp;\");\n",
      "\t\n",
      "\t\n",
      "\t//设置下一页代码 \n",
      "\tif(countPage>1&&currentPage!=(countPage-1))\n",
      "\t\tdocument.write(\"<a href=\\\"t20090818_27775\"+\"_\" + 下一页Page + \".\"+\"html\\\" ><span style=\\\"color:#0033FF;font-weight:bold\\\">下一页</span></a> &nbsp;\");\n",
      "\t//else\n",
      "\t//\tdocument.write(\"下一页 &nbsp;\");\n",
      "\t\t\t\t\t \n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "print scripts.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countPage = int(''.join(scripts).split('countPage = ')[1].split('//')[0])\n",
    "countPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawler(url_i):\n",
    "    content = urllib2.urlopen(url_i).read().decode('gb18030')  \n",
    "    soup = BeautifulSoup(content, 'html.parser') \n",
    "    year = soup.find('span', {'class', 'huang16c'}).text[:4]\n",
    "    year = int(year)\n",
    "    report = ''.join(s.text for s in soup('p'))\n",
    "    scripts = soup.find_all('script')\n",
    "    countPage = int(''.join(scripts[1]).split('countPage = ')[1].split('//')[0])\n",
    "    if countPage == 1:\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(1, countPage):\n",
    "            url_child = url_i.split('.html')[0] +'_'+str(i)+'.html'\n",
    "            content = urllib2.urlopen(url_child).read().decode('gb18030') \n",
    "            soup = BeautifulSoup(content) \n",
    "            report_child = ''.join(s.text for s in soup('p'))\n",
    "            report = report + report_child\n",
    "    return year, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuyulunbi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "2010\n",
      "2009\n",
      "2008\n",
      "2007\n",
      "2006\n",
      "2005\n",
      "2004\n",
      "2003\n",
      "2002\n",
      "2001\n",
      "2000\n",
      "1999\n",
      "1998\n",
      "1997\n",
      "1996\n",
      "1995\n",
      "1994\n",
      "1993\n",
      "1992\n",
      "1991\n",
      "1990\n",
      "1989\n",
      "1988\n",
      "1987\n",
      "1986\n",
      "1985\n",
      "1984\n",
      "1983\n",
      "1982\n",
      "1981\n",
      "1980\n",
      "1979\n",
      "1978\n",
      "1975\n",
      "1964\n",
      "1959\n",
      "1960\n",
      "1957\n",
      "1956\n",
      "1955\n",
      "1954\n"
     ]
    }
   ],
   "source": [
    "reports = {}\n",
    "for link in hyperlinks:\n",
    "    year, report = crawler(link)\n",
    "    print year\n",
    "    reports[year] = report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url2016 = 'http://news.xinhuanet.com/fortune/2016-03/05/c_128775704.htm'\n",
    "content = urllib2.urlopen(url2016).read()\n",
    "soup = BeautifulSoup(content, 'html.parser') \n",
    "report2016 = ''.join(s.text for s in soup('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/wuyulunbi/Desktop/gov_reports1954-2016.txt', 'wb') as f:\n",
    "    for r in reports:\n",
    "        line = str(r)+'\\t'+reports[r].replace('\\n', '\\t') +'\\n'\n",
    "        f.write(line.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawler(page_num, file_name):\n",
    "    try:\n",
    "        url = \"http://bbs.tianya.cn/list.jsp?item=free&nextid=%d&order=8&k=婚姻\" % page_num\n",
    "        content = urllib2.urlopen(url).read() \n",
    "        soup = BeautifulSoup(content, \"lxml\") \n",
    "        articles = soup.find_all('tr')\n",
    "        for i in articles[1:]:\n",
    "            td = i.find_all('td')\n",
    "            title = td[0].text.strip()\n",
    "            title_url = td[0].a['href']\n",
    "            author = td[1].text\n",
    "            author_url = td[1].a['href']\n",
    "            views = td[2].text\n",
    "            replies = td[3].text\n",
    "            date = td[4]['title']\n",
    "            record = title + '\\t' + title_url+ '\\t' + author + '\\t'+ \\\n",
    "                        author_url + '\\t' + views+ '\\t'  + replies+ '\\t'+ date\n",
    "            with open(file_name,'a') as p: \n",
    "                        p.write(record.encode('utf-8')+\"\\n\") \n",
    "    except Exception, e:\n",
    "        print e\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for page_num in range(10):\n",
    "    print (page_num)\n",
    "    crawler(page_num, '/Users/wuyulunbi/Desktop/tianya_bbs_threads_list.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>告诉你一个残酷的真相:婚姻劈腿</td>\n",
       "      <td>/post-free-1997937-1.shtml</td>\n",
       "      <td>kure1010</td>\n",
       "      <td>http://www.tianya.cn/22030005</td>\n",
       "      <td>898864</td>\n",
       "      <td>3467</td>\n",
       "      <td>2010-10-15 22:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我在越南涉外婚姻介绍所坐翻译的那些事(连载)</td>\n",
       "      <td>/post-free-1503943-1.shtml</td>\n",
       "      <td>越来越南09</td>\n",
       "      <td>http://www.tianya.cn/21953475</td>\n",
       "      <td>476391</td>\n",
       "      <td>2130</td>\n",
       "      <td>2009-02-13 12:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0                           1         2  \\\n",
       "0         告诉你一个残酷的真相:婚姻劈腿  /post-free-1997937-1.shtml  kure1010   \n",
       "1  我在越南涉外婚姻介绍所坐翻译的那些事(连载)  /post-free-1503943-1.shtml    越来越南09   \n",
       "\n",
       "                               3       4     5                 6  \n",
       "0  http://www.tianya.cn/22030005  898864  3467  2010-10-15 22:04  \n",
       "1  http://www.tianya.cn/21953475  476391  2130  2009-02-13 12:53  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/wuyulunbi/Desktop/tianya_bbs_threads_list.txt', sep = \"\\t\", header=None)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>author_page</th>\n",
       "      <th>click</th>\n",
       "      <th>reply</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>告诉你一个残酷的真相:婚姻劈腿</td>\n",
       "      <td>/post-free-1997937-1.shtml</td>\n",
       "      <td>kure1010</td>\n",
       "      <td>http://www.tianya.cn/22030005</td>\n",
       "      <td>898864</td>\n",
       "      <td>3467</td>\n",
       "      <td>2010-10-15 22:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我在越南涉外婚姻介绍所坐翻译的那些事(连载)</td>\n",
       "      <td>/post-free-1503943-1.shtml</td>\n",
       "      <td>越来越南09</td>\n",
       "      <td>http://www.tianya.cn/21953475</td>\n",
       "      <td>476391</td>\n",
       "      <td>2130</td>\n",
       "      <td>2009-02-13 12:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                        link    author  \\\n",
       "0         告诉你一个残酷的真相:婚姻劈腿  /post-free-1997937-1.shtml  kure1010   \n",
       "1  我在越南涉外婚姻介绍所坐翻译的那些事(连载)  /post-free-1503943-1.shtml    越来越南09   \n",
       "\n",
       "                     author_page   click  reply              time  \n",
       "0  http://www.tianya.cn/22030005  898864   3467  2010-10-15 22:04  \n",
       "1  http://www.tianya.cn/21953475  476391   2130  2009-02-13 12:53  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.rename(columns = {0:'title', 1:'link', 2:'author',3:'author_page', 4:'click', 5:'reply', 6:'time'})\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    http://www.tianya.cn/22030005\n",
       "1    http://www.tianya.cn/21953475\n",
       "2    http://www.tianya.cn/11341192\n",
       "3    http://www.tianya.cn/14067214\n",
       "4    http://www.tianya.cn/28226296\n",
       "Name: author_page, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author_page[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = df.author_page[1]\n",
    "content = urllib2.urlopen(url).read() \n",
    "soup1 = BeautifulSoup(content, \"lxml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def author_crawler(url, file_name):\n",
    "    try:\n",
    "        content = urllib2.urlopen(url).read() \n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        link_info = soup.find_all('div', {'class', 'link-box'})\n",
    "        followed_num, fans_num = [i.a.text for i in link_info]\n",
    "        try:\n",
    "            activity = soup.find_all('span', {'class', 'subtitle'})\n",
    "            post_num, reply_num = [j.text[2:] for i in activity[:1] for j in i('a')]\n",
    "        except:\n",
    "            post_num, reply_num = 1, 0\n",
    "        record =  '\\t'.join([url, followed_num, fans_num, post_num, reply_num])\n",
    "        with open(file_name,'a') as p: \n",
    "                    p.write(record.encode('utf-8')+\"\\n\") \n",
    "\n",
    "    except Exception, e:\n",
    "        print e, url\n",
    "        record =  '\\t'.join([url, 'na', 'na', 'na', 'na'])\n",
    "        with open(file_name,'a') as p: \n",
    "                    p.write(record.encode('utf-8')+\"\\n\") \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "need more than 0 values to unpack http://www.tianya.cn/51647103\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "need more than 0 values to unpack http://www.tianya.cn/99218216\n",
      "need more than 0 values to unpack http://www.tianya.cn/95540801\n",
      "100\n",
      "110\n",
      "need more than 0 values to unpack http://www.tianya.cn/104679587\n",
      "120\n",
      "130\n",
      "need more than 0 values to unpack http://www.tianya.cn/2111083\n",
      "140\n",
      "150\n",
      "need more than 0 values to unpack http://www.tianya.cn/51647103\n",
      "160\n",
      "need more than 0 values to unpack http://www.tianya.cn/13588445\n",
      "170\n",
      "180\n",
      "need more than 0 values to unpack http://www.tianya.cn/14422795\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "need more than 0 values to unpack http://www.tianya.cn/10052346\n",
      "260\n",
      "270\n",
      "need more than 0 values to unpack http://www.tianya.cn/5980473\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "need more than 0 values to unpack http://www.tianya.cn/5761853\n",
      "320\n",
      "need more than 0 values to unpack http://www.tianya.cn/75261644\n",
      "330\n",
      "need more than 0 values to unpack http://www.tianya.cn/3542002\n",
      "340\n",
      "350\n",
      "need more than 0 values to unpack http://www.tianya.cn/38810323\n",
      "360\n",
      "370\n",
      "380\n",
      "need more than 0 values to unpack http://www.tianya.cn/2360176\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "need more than 0 values to unpack http://www.tianya.cn/16158988\n",
      "need more than 0 values to unpack http://www.tianya.cn/2461333\n",
      "430\n",
      "sequence item 3: expected string or Unicode, int found http://www.tianya.cn/34029033\n",
      "need more than 0 values to unpack http://www.tianya.cn/7478072\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "sequence item 3: expected string or Unicode, int found http://www.tianya.cn/13531307\n",
      "480\n",
      "need more than 0 values to unpack http://www.tianya.cn/17294023\n",
      "sequence item 3: expected string or Unicode, int found http://www.tianya.cn/7055500\n",
      "sequence item 3: expected string or Unicode, int found http://www.tianya.cn/53548405\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "for k, url in enumerate(df.author_page):\n",
    "    if k % 10==0:\n",
    "        print k\n",
    "    author_crawler(url, '/Users/wuyulunbi/Desktop/tianya_bbs_threads_author_info.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
